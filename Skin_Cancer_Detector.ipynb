{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Training Code**","metadata":{}},{"cell_type":"code","source":"# Importing Libraries\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Input\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# Importing Tabular Data (Metadata)\n\ntabular_data = pd.read_csv('/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv')\n\nprint(tabular_data.head())\n\n# Importing Image Data with Pixel Values and Labels\n\ndata = pd.read_csv('/kaggle/input/skin-cancer-mnist-ham10000/hmnist_28_28_RGB.csv')\n\n# Splitting data into features (x) and labels (y)\n\nx = data.drop('label', axis=1)\ny = data['label']\n\n# Oversampling to Overcome Class Imbalance\n\noversample = RandomOverSampler(random_state=42)\nx_resampled, y_resampled = oversample.fit_resample(x, y)\n\n# Reshaping x to match image dimensions (28, 28, 3)\n\nx_resampled = np.array(x_resampled).reshape(-1, 28, 28, 3)\n\nprint('Shape of x after oversampling and reshaping:', x_resampled.shape)\n\n# Standardizing Data\n\nmean = np.mean(x_resampled)\nstd = np.std(x_resampled)\nx_resampled = (x_resampled - mean) / std\n\n# Splitting Data into Training and Testing Sets\n\nX_train, X_test, Y_train, Y_test = train_test_split(\n    x_resampled, y_resampled, test_size=0.2, random_state=1\n)\n\nprint(f'Training set size: {X_train.shape}, Testing set size: {X_test.shape}')\n\n# Building the CNN Model\n\nmodel = Sequential([\n    Input(shape=(28, 28, 3)),  # Adjusted input shape to match data\n    Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'),\n    Conv2D(32, kernel_size=(3, 3), activation='relu'),\n    MaxPool2D(pool_size=(2, 2)),\n    Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n    MaxPool2D(pool_size=(2, 2), padding='same'),\n    Flatten(),\n    Dense(64, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(7, activation='softmax')\n])\n\nmodel.summary()\n\n# Callback to save the best model\n\ncallback = ModelCheckpoint(\n    filepath='best_model.keras',  # Changed to .keras extension\n    monitor='val_accuracy',       # Monitoring validation accuracy\n    mode='max',\n    save_best_only=True,\n    verbose=1\n)\n\n# Compiling the Model\n\nmodel.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\n# Training the Model\n\nhistory = model.fit(\n    X_train, Y_train,\n    validation_split=0.2,\n    batch_size=128,\n    epochs=20,\n    callbacks=[callback]\n)\n\nmodel.save('final_model.h5')\nprint(\"Saved 'final_model.h5'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:14:50.247433Z","iopub.execute_input":"2024-11-29T07:14:50.247782Z","iopub.status.idle":"2024-11-29T07:15:25.744227Z","shell.execute_reply.started":"2024-11-29T07:14:50.247753Z","shell.execute_reply":"2024-11-29T07:15:25.743343Z"}},"outputs":[{"name":"stdout","text":"     lesion_id      image_id   dx dx_type   age   sex localization\n0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear\nShape of x after oversampling and reshaping: (46935, 28, 28, 3)\nTraining set size: (37548, 28, 28, 3), Testing set size: (9387, 28, 28, 3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m448\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m147,520\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m231\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,520</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">231</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m182,663\u001b[0m (713.53 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">182,663</span> (713.53 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m182,663\u001b[0m (713.53 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">182,663</span> (713.53 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4161 - loss: 1.4697\nEpoch 1: val_accuracy improved from -inf to 0.71265, saving model to best_model.keras\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.4166 - loss: 1.4685 - val_accuracy: 0.7126 - val_loss: 0.7790\nEpoch 2/20\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7324 - loss: 0.7139\nEpoch 2: val_accuracy improved from 0.71265 to 0.81065, saving model to best_model.keras\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7326 - loss: 0.7136 - val_accuracy: 0.8107 - val_loss: 0.5007\nEpoch 3/20\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8349 - loss: 0.4479\nEpoch 3: val_accuracy improved from 0.81065 to 0.85672, saving model to best_model.keras\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8350 - loss: 0.4477 - val_accuracy: 0.8567 - val_loss: 0.3710\nEpoch 4/20\n\u001b[1m229/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8852 - loss: 0.3129\nEpoch 4: val_accuracy improved from 0.85672 to 0.88415, saving model to best_model.keras\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8855 - loss: 0.3121 - val_accuracy: 0.8842 - val_loss: 0.3099\nEpoch 5/20\n\u001b[1m225/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9176 - loss: 0.2256\nEpoch 5: val_accuracy improved from 0.88415 to 0.91425, saving model to best_model.keras\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9179 - loss: 0.2247 - val_accuracy: 0.9142 - val_loss: 0.2294\nEpoch 6/20\n\u001b[1m232/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9399 - loss: 0.1647\nEpoch 6: val_accuracy improved from 0.91425 to 0.92144, saving model to best_model.keras\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9400 - loss: 0.1646 - val_accuracy: 0.9214 - val_loss: 0.2086\nEpoch 7/20\n\u001b[1m233/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9505 - loss: 0.1386\nEpoch 7: val_accuracy improved from 0.92144 to 0.92210, saving model to best_model.keras\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9505 - loss: 0.1386 - val_accuracy: 0.9221 - val_loss: 0.2067\nEpoch 8/20\n\u001b[1m226/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9567 - loss: 0.1160\nEpoch 8: val_accuracy improved from 0.92210 to 0.94527, saving model to best_model.keras\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9568 - loss: 0.1159 - val_accuracy: 0.9453 - val_loss: 0.1951\nEpoch 9/20\n\u001b[1m231/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9658 - loss: 0.0994\nEpoch 9: val_accuracy improved from 0.94527 to 0.95353, saving model to best_model.keras\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9658 - loss: 0.0994 - val_accuracy: 0.9535 - val_loss: 0.1525\nEpoch 10/20\n\u001b[1m229/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9746 - loss: 0.0742\nEpoch 10: val_accuracy improved from 0.95353 to 0.95646, saving model to best_model.keras\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9745 - loss: 0.0743 - val_accuracy: 0.9565 - val_loss: 0.1639\nEpoch 11/20\n\u001b[1m223/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9794 - loss: 0.0575\nEpoch 11: val_accuracy did not improve from 0.95646\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9795 - loss: 0.0574 - val_accuracy: 0.9529 - val_loss: 0.2057\nEpoch 12/20\n\u001b[1m225/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9729 - loss: 0.0740\nEpoch 12: val_accuracy did not improve from 0.95646\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9729 - loss: 0.0741 - val_accuracy: 0.9529 - val_loss: 0.1942\nEpoch 13/20\n\u001b[1m223/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9840 - loss: 0.0469\nEpoch 13: val_accuracy did not improve from 0.95646\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9839 - loss: 0.0472 - val_accuracy: 0.9486 - val_loss: 0.1790\nEpoch 14/20\n\u001b[1m228/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9848 - loss: 0.0439\nEpoch 14: val_accuracy improved from 0.95646 to 0.95779, saving model to best_model.keras\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9847 - loss: 0.0441 - val_accuracy: 0.9578 - val_loss: 0.1564\nEpoch 15/20\n\u001b[1m224/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9788 - loss: 0.0608\nEpoch 15: val_accuracy improved from 0.95779 to 0.95992, saving model to best_model.keras\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9788 - loss: 0.0607 - val_accuracy: 0.9599 - val_loss: 0.1745\nEpoch 16/20\n\u001b[1m227/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9853 - loss: 0.0415\nEpoch 16: val_accuracy improved from 0.95992 to 0.96205, saving model to best_model.keras\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9854 - loss: 0.0413 - val_accuracy: 0.9621 - val_loss: 0.1983\nEpoch 17/20\n\u001b[1m233/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9893 - loss: 0.0286\nEpoch 17: val_accuracy improved from 0.96205 to 0.96778, saving model to best_model.keras\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9893 - loss: 0.0286 - val_accuracy: 0.9678 - val_loss: 0.1786\nEpoch 18/20\n\u001b[1m224/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9888 - loss: 0.0312\nEpoch 18: val_accuracy did not improve from 0.96778\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9886 - loss: 0.0318 - val_accuracy: 0.9655 - val_loss: 0.1656\nEpoch 19/20\n\u001b[1m223/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9824 - loss: 0.0497\nEpoch 19: val_accuracy did not improve from 0.96778\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9825 - loss: 0.0496 - val_accuracy: 0.9675 - val_loss: 0.1560\nEpoch 20/20\n\u001b[1m226/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9924 - loss: 0.0238\nEpoch 20: val_accuracy improved from 0.96778 to 0.97483, saving model to best_model.keras\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9924 - loss: 0.0236 - val_accuracy: 0.9748 - val_loss: 0.1584\nSaved 'final_model.h5'\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# **Testing Code**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom PIL import Image\nimport os\n\n# Set seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Define the correct class labels mapping\nclass_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc', 'mel']\n# Alternatively, use a dictionary\nlabels_dict = {\n    0: 'akiec',\n    1: 'bcc',\n    2: 'bkl',\n    3: 'df',\n    4: 'nv',\n    5: 'vasc',\n    6: 'mel'\n}\n\nprint(\"Class labels:\", class_labels)\n\n# Load the trained model (ensure you're using the correct model file)\nmodel = tf.keras.models.load_model('final_model.h5')  # Use the correct model filename\n\n# Load mean and std from training\nmean = np.load('mean.npy')\nstd = np.load('std.npy')\n\n# Define the image preprocessing function\ndef preprocess_image(image):\n    image = image.resize((28, 28))  # PIL uses (width, height)\n    image = np.asarray(image)\n    print(f'Image shape after resizing: {image.shape}')  # Should output (28, 28, 3)\n    image = (image - mean) / std  # Standardize the image\n    image = np.expand_dims(image, axis=0)  # Add batch dimension\n    return image\n\n# Path to the image you want to test\nimage_path = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2/ISIC_0029486.jpg'\n\n# Check if the image exists\nif not os.path.isfile(image_path):\n    raise FileNotFoundError(f\"The image file '{image_path}' does not exist.\")\n\n# Open and preprocess the image\nimage = Image.open(image_path).convert('RGB')  # Ensure image has 3 channels\nprocessed_image = preprocess_image(image)\n\n# Make prediction\npredictions = model.predict(processed_image)\n\n# Get predicted class index and name\npredicted_class_idx = np.argmax(predictions, axis=1)[0]\n\n# Use the correct mapping to get the class name\npredicted_class = class_labels[predicted_class_idx]\n# Or if using labels_dict:\n# predicted_class = labels_dict[predicted_class_idx]\n\nconfidence = predictions[0][predicted_class_idx]\n\n# Print result\nprint(f\"Predicted Class: {predicted_class}\")\nprint(f\"Confidence: {confidence}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:38:57.864712Z","iopub.execute_input":"2024-11-29T07:38:57.865038Z","iopub.status.idle":"2024-11-29T07:38:58.258550Z","shell.execute_reply.started":"2024-11-29T07:38:57.865007Z","shell.execute_reply":"2024-11-29T07:38:58.257710Z"}},"outputs":[{"name":"stdout","text":"Class labels: ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc', 'mel']\nImage shape after resizing: (28, 28, 3)\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\nPredicted Class: vasc\nConfidence: 1.0\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# **Testing Code on all images**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm  # For progress bar\n\n# Set seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Define the correct class labels mapping\nclass_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc', 'mel']\nlabel_to_index = {label: idx for idx, label in enumerate(class_labels)}\nindex_to_label = {idx: label for idx, label in enumerate(class_labels)}\n\nprint(\"Class labels:\", class_labels)\n\n# Load the trained model\nmodel = tf.keras.models.load_model('final_model.h5')  # Ensure this filename matches your trained model\n\n# Load mean and std from training\nmean = np.load('mean.npy')\nstd = np.load('std.npy')\n\n# Define the image preprocessing function\ndef preprocess_image(image_path):\n    image = Image.open(image_path).convert('RGB')  # Ensure image has 3 channels\n    image = image.resize((28, 28))  # PIL uses (width, height)\n    image = np.asarray(image)\n    image = (image - mean) / std  # Standardize the image\n    image = np.expand_dims(image, axis=0)  # Add batch dimension\n    return image\n\n# Paths to the image directories\nimage_dir_1 = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1/'\nimage_dir_2 = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2/'\n\n# Load the metadata\nmetadata = pd.read_csv('/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv')\n\n# Mapping of image IDs to file paths\nimageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n                     for x in list(\n    map(lambda x: os.path.join(image_dir_1, x), os.listdir(image_dir_1))\n) + list(\n    map(lambda x: os.path.join(image_dir_2, x), os.listdir(image_dir_2))\n)}\n\n# Add file paths to the DataFrame\nmetadata['image_path'] = metadata['image_id'].map(imageid_path_dict.get)\n\n# Map diagnostic labels to numerical labels\nmetadata['true_label'] = metadata['dx'].map(label_to_index.get)\n\n# Initialize counters\ntotal_images = len(metadata)\ncorrect_predictions = 0\nincorrect_predictions = 0\n\n# Initialize list to store prediction results\nresults = []\n\n# Loop over each image in the metadata\nfor idx, row in tqdm(metadata.iterrows(), total=total_images, ncols=80, leave=True, position=0):\n    image_path = row['image_path']\n    true_label_idx = row['true_label']\n    true_label_name = index_to_label[true_label_idx]\n\n    # Preprocess image\n    processed_image = preprocess_image(image_path)\n\n    # Make prediction without printing any progress bar\n    predictions = model.predict(processed_image, verbose=0)\n    confidence = np.max(predictions)\n\n    # Get predicted class index and name\n    predicted_class_idx = np.argmax(predictions, axis=1)[0]\n    predicted_class_name = index_to_label[predicted_class_idx]\n\n    # Compare prediction with true label\n    if predicted_class_idx == true_label_idx:\n        correct_predictions += 1\n    else:\n        incorrect_predictions += 1\n\n    # Store the result\n    results.append({\n        'image_id': row['image_id'],\n        'true_label': true_label_name,\n        'predicted_label': predicted_class_name,\n        'confidence': confidence\n    })\n\n# Report results\nprint(f\"\\nTotal Images: {total_images}\")\nprint(f\"Correct Predictions: {correct_predictions}\")\nprint(f\"Incorrect Predictions: {incorrect_predictions}\")\nprint(f\"Accuracy: {correct_predictions / total_images * 100:.2f}%\")\n\n# Create DataFrame from results\nresults_df = pd.DataFrame(results)\n\n# Save results to CSV\nresults_df.to_csv('prediction_results.csv', index=False)\nprint(\"Saved prediction results to 'prediction_results.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:08:25.496732Z","iopub.execute_input":"2024-11-29T08:08:25.497415Z","iopub.status.idle":"2024-11-29T08:17:59.536667Z","shell.execute_reply.started":"2024-11-29T08:08:25.497377Z","shell.execute_reply":"2024-11-29T08:17:59.535817Z"}},"outputs":[{"name":"stdout","text":"Class labels: ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc', 'mel']\n","output_type":"stream"},{"name":"stderr","text":"100%|█████████████████████████████████████| 10015/10015 [09:33<00:00, 17.45it/s]","output_type":"stream"},{"name":"stdout","text":"\nTotal Images: 10015\nCorrect Predictions: 9471\nIncorrect Predictions: 544\nAccuracy: 94.57%\nSaved prediction results to 'prediction_results.csv'\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":32}]}